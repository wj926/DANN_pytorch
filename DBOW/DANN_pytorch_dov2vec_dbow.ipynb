{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable, Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size=2500\n",
    "test_size=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/home/tako/saerom/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src='kitchen'\n",
    "tgt='dvd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_path+'doc2vec'+'_source_'+src+'_target_'+tgt+'_dbow_data.pickle','rb')\n",
    "aa = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xs_train = aa['docvec'][aa['st_label']==1][:train_size]\n",
    "ys_train = (aa['true_label'][aa['st_label']==1][:train_size] + 1)/2\n",
    "Xt_train = aa['docvec'][aa['st_label']==0][:train_size]\n",
    "yt_train = (aa['true_label'][aa['st_label']==0][:train_size] + 1)/2\n",
    "Xs_test = aa['docvec'][aa['st_label']==1][-test_size:]\n",
    "ys_test = (aa['true_label'][aa['st_label']==1][-test_size:] +1 )/2\n",
    "Xt_test = aa['docvec'][aa['st_label']==0][-test_size:]\n",
    "yt_test = (aa['true_label'][aa['st_label']==0][-test_size:] +1 )/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imput_dim = Xs_train.shape[1]\n",
    "hidden_dim =200\n",
    "feature_dim =100\n",
    "classnum=1\n",
    "\n",
    "learning_rate = 1e-2          # learning rate\n",
    "num_epochs = 100               # number of epochs to train models\n",
    "batch_size = 100               # size of image sample per epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load source domain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_func(Xt_test, yt_test):\n",
    "    out = c_clf(f_ext(Variable(torch.from_numpy(Xt_test[:len(Xt_test)]).float())).view(len(Xt_test), -1))\n",
    "    right=0\n",
    "    for i in range(len(out)):\n",
    "        if out.data.cpu().numpy()[i] - yt_test[i]<0.5:\n",
    "           right += 1\n",
    "    return right/len(Xt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(batch_size, data, labels):\n",
    "    size = data.shape[0]\n",
    "    idx_array = np.arange(size)\n",
    "    n_batch = int(np.ceil(size / float(batch_size)))\n",
    "    batches = [(int(i * batch_size), int(min(size, (i + 1) * batch_size))) for i in range(n_batch)]\n",
    "    for batch_index, (start, end) in enumerate(batches):\n",
    "        print('\\rBatch {}/{}'.format(batch_index+1, n_batch), end='')\n",
    "        batch_ids = idx_array[start:end]\n",
    "        if labels is not None:\n",
    "            yield Variable(torch.from_numpy(data[batch_ids])), Variable(torch.from_numpy(labels[batch_ids])), batch_ids\n",
    "        else:\n",
    "            yield Variable(torch.from_numpy(data[batch_ids])), batch_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_clf(model1, model2, x, y, n):\n",
    "    out = model1(model2(Variable(torch.from_numpy(x[:n]).float())).view(n, -1))\n",
    "    preds = out.max(1)[1]\n",
    "    return accuracy_score(y_true=[np.argmax(i) for i in y[:n]], y_pred=preds.data.numpy().ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DANN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GradReverse(Function):\n",
    "    def __init__(self, lambd):\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return (grad_output * -self.lambd)\n",
    "\n",
    "def grad_reverse(x, lambd):\n",
    "    return GradReverse(lambd)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class feature_extract(nn.Module):\n",
    "    def __init__(self, imput_dim, feature_dim):\n",
    "        super(feature_extract, self).__init__()\n",
    "        self.fc1 = nn.Linear(imput_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dann_domain_clf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dann_domain_clf, self).__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 100) \n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "\n",
    "    def set_lambda(self, lambd):\n",
    "        self.lambd = lambd\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = grad_reverse(x, self.lambd)\n",
    "        x = F.leaky_relu(self.drop(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dann_class_clf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dann_class_clf, self).__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, classnum)\n",
    "        self.drop = nn.Dropout2d(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.drop(self.fc1(x)))\n",
    "        x = F.relu(self.drop(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_ext, d_clf, c_clf = feature_extract(imput_dim, feature_dim), dann_domain_clf(), dann_class_clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_crit = nn.BCELoss()\n",
    "c_crit = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_optimizer = optim.SGD(d_clf.parameters(), lr=learning_rate, momentum=0.9)\n",
    "c_optimizer = optim.SGD(c_clf.parameters(), lr=learning_rate, momentum=0.9)\n",
    "f_optimizer = optim.SGD(f_ext.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = num_epochs * (Xs_train.shape[0] / batch_size)\n",
    "yd = Variable(torch.from_numpy(np.hstack([np.repeat(1, int(batch_size / 2)), np.repeat(0, int(batch_size / 2))]).reshape(batch_size, 1)))\n",
    "j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DANN model..\n",
      "\r",
      "Batch 1/50\r",
      "Batch 1/50\r",
      "Epoch         - d_loss: 0.6952 - c_loss: 0.6941\r",
      "Batch 2/50\r",
      "Batch 2/50\r",
      "Epoch         - d_loss: 0.6946 - c_loss: 0.6901\r",
      "Batch 3/50\r",
      "Batch 3/50\r",
      "Epoch         - d_loss: 0.6938 - c_loss: 0.6923\r",
      "Batch 4/50\r",
      "Batch 4/50\r",
      "Epoch         - d_loss: 0.6939 - c_loss: 0.6930\r",
      "Batch 5/50\r",
      "Batch 5/50\r",
      "Epoch         - d_loss: 0.6920 - c_loss: 0.6910\r",
      "Batch 6/50\r",
      "Batch 6/50\r",
      "Epoch         - d_loss: 0.6935 - c_loss: 0.6948\r",
      "Batch 7/50\r",
      "Batch 7/50\r",
      "Epoch         - d_loss: 0.6932 - c_loss: 0.6967\r",
      "Batch 8/50\r",
      "Batch 8/50\r",
      "Epoch         - d_loss: 0.6946 - c_loss: 0.6923\r",
      "Batch 9/50\r",
      "Batch 9/50\r",
      "Epoch         - d_loss: 0.6930 - c_loss: 0.6918\r",
      "Batch 10/50\r",
      "Batch 10/50\r",
      "Epoch         - d_loss: 0.6946 - c_loss: 0.6969\r",
      "Batch 11/50\r",
      "Batch 11/50\r",
      "Epoch         - d_loss: 0.6953 - c_loss: 0.6903\r",
      "Batch 12/50\r",
      "Batch 12/50\r",
      "Epoch         - d_loss: 0.6929 - c_loss: 0.6933\r",
      "Batch 13/50\r",
      "Batch 13/50\r",
      "Epoch         - d_loss: 0.6941 - c_loss: 0.6911\r",
      "Batch 14/50\r",
      "Batch 14/50\r",
      "Epoch         - d_loss: 0.6938 - c_loss: 0.6920\r",
      "Batch 15/50\r",
      "Batch 15/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tako/anaconda3/envs/pytorch2/lib/python3.6/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([50, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch         - d_loss: 0.6927 - c_loss: 0.6929 - target_acc: 0.7800 - source_acc: 0.7744\n",
      "Epoch         - d_loss: 0.6937 - c_loss: 0.6928 - target_acc: 0.7884 - source_acc: 0.8068\n",
      "Epoch         - d_loss: 0.6933 - c_loss: 0.6916 - target_acc: 0.8648 - source_acc: 0.8684\n",
      "Epoch         - d_loss: 0.6920 - c_loss: 0.6912 - target_acc: 0.8752 - source_acc: 0.8736\n",
      "Epoch         - d_loss: 0.6932 - c_loss: 0.6929 - target_acc: 0.8840 - source_acc: 0.8840\n",
      "Epoch         - d_loss: 0.6935 - c_loss: 0.6888 - target_acc: 0.8892 - source_acc: 0.9012\n",
      "Epoch         - d_loss: 0.6957 - c_loss: 0.6894 - target_acc: 0.8932 - source_acc: 0.9064\n",
      "Epoch         - d_loss: 0.6948 - c_loss: 0.6868 - target_acc: 0.8900 - source_acc: 0.9044\n",
      "Epoch         - d_loss: 0.6945 - c_loss: 0.6692 - target_acc: 0.9288 - source_acc: 0.9476\n",
      "Epoch         - d_loss: 0.6948 - c_loss: 0.6264 - target_acc: 0.9484 - source_acc: 0.9652\n",
      "Epoch         - d_loss: 0.6945 - c_loss: 0.4980 - target_acc: 0.9320 - source_acc: 0.9412\n",
      "Epoch         - d_loss: 0.6950 - c_loss: 0.4045 - target_acc: 0.9200 - source_acc: 0.9372\n",
      "Epoch         - d_loss: 0.6939 - c_loss: 0.3782 - target_acc: 0.9016 - source_acc: 0.9316\n",
      "Epoch         - d_loss: 0.6925 - c_loss: 0.3440 - target_acc: 0.8940 - source_acc: 0.9336\n",
      "Epoch         - d_loss: 0.6914 - c_loss: 0.3176 - target_acc: 0.8916 - source_acc: 0.9444\n",
      "Epoch         - d_loss: 0.6926 - c_loss: 0.2987 - target_acc: 0.9072 - source_acc: 0.9620\n",
      "Epoch         - d_loss: 0.6924 - c_loss: 0.2678 - target_acc: 0.8864 - source_acc: 0.9656\n",
      "Epoch         - d_loss: 0.6932 - c_loss: 0.2585 - target_acc: 0.8884 - source_acc: 0.9772\n",
      "Epoch         - d_loss: 0.6940 - c_loss: 0.2563 - target_acc: 0.8616 - source_acc: 0.9760\n",
      "Epoch         - d_loss: 0.6920 - c_loss: 0.2026 - target_acc: 0.8724 - source_acc: 0.9844\n",
      "Epoch         - d_loss: 0.6922 - c_loss: 0.1493 - target_acc: 0.9076 - source_acc: 0.9940\n",
      "Epoch         - d_loss: 0.6910 - c_loss: 0.2180 - target_acc: 0.9296 - source_acc: 0.9956\n",
      "Epoch         - d_loss: 0.6888 - c_loss: 0.1777 - target_acc: 0.9480 - source_acc: 0.9984\n",
      "Epoch         - d_loss: 0.6892 - c_loss: 0.0686 - target_acc: 0.8352 - source_acc: 0.9828\n",
      "Epoch         - d_loss: 0.6922 - c_loss: 0.0920 - target_acc: 0.8984 - source_acc: 0.9980\n",
      "Epoch         - d_loss: 0.6900 - c_loss: 0.0842 - target_acc: 0.9448 - source_acc: 0.9988\n",
      "Epoch         - d_loss: 0.6945 - c_loss: 0.0995 - target_acc: 0.9604 - source_acc: 0.9992\n",
      "Epoch         - d_loss: 0.6884 - c_loss: 0.0920 - target_acc: 0.8984 - source_acc: 0.9984\n",
      "Epoch         - d_loss: 0.6917 - c_loss: 0.0078 - target_acc: 0.8828 - source_acc: 0.9984\n",
      "Epoch         - d_loss: 0.6911 - c_loss: 0.0115 - target_acc: 0.9076 - source_acc: 0.9992\n",
      "Epoch         - d_loss: 0.6916 - c_loss: 0.0070 - target_acc: 0.9232 - source_acc: 0.9992\n",
      "Epoch         - d_loss: 0.6919 - c_loss: 0.0047 - target_acc: 0.8996 - source_acc: 0.9996\n",
      "Epoch         - d_loss: 0.6894 - c_loss: 0.0024 - target_acc: 0.8912 - source_acc: 0.9996\n",
      "Epoch         - d_loss: 0.6932 - c_loss: 0.0015 - target_acc: 0.9164 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6936 - c_loss: 0.0038 - target_acc: 0.9108 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6877 - c_loss: 0.0013 - target_acc: 0.9140 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6882 - c_loss: 0.0011 - target_acc: 0.8764 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6924 - c_loss: 0.0019 - target_acc: 0.8696 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6944 - c_loss: 0.0021 - target_acc: 0.9044 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6929 - c_loss: 0.0017 - target_acc: 0.9148 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6902 - c_loss: 0.0012 - target_acc: 0.8924 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6891 - c_loss: 0.0013 - target_acc: 0.8852 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6969 - c_loss: 0.0007 - target_acc: 0.9172 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6992 - c_loss: 0.0008 - target_acc: 0.9136 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6944 - c_loss: 0.0009 - target_acc: 0.8916 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6928 - c_loss: 0.0010 - target_acc: 0.8888 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6940 - c_loss: 0.0006 - target_acc: 0.9168 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6930 - c_loss: 0.0013 - target_acc: 0.9088 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6898 - c_loss: 0.0013 - target_acc: 0.8800 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6874 - c_loss: 0.0003 - target_acc: 0.8900 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6950 - c_loss: 0.0019 - target_acc: 0.9196 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6921 - c_loss: 0.0008 - target_acc: 0.9144 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6951 - c_loss: 0.0005 - target_acc: 0.8800 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6922 - c_loss: 0.0003 - target_acc: 0.8964 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6886 - c_loss: 0.0012 - target_acc: 0.9200 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6958 - c_loss: 0.0002 - target_acc: 0.8868 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6969 - c_loss: 0.0002 - target_acc: 0.8692 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6937 - c_loss: 0.0004 - target_acc: 0.8984 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6865 - c_loss: 0.0005 - target_acc: 0.9012 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6885 - c_loss: 0.0002 - target_acc: 0.8872 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6976 - c_loss: 0.0001 - target_acc: 0.8992 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6913 - c_loss: 0.0006 - target_acc: 0.9100 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6887 - c_loss: 0.0002 - target_acc: 0.8892 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6931 - c_loss: 0.0005 - target_acc: 0.8844 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6960 - c_loss: 0.0005 - target_acc: 0.9152 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6941 - c_loss: 0.0003 - target_acc: 0.9120 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6913 - c_loss: 0.0002 - target_acc: 0.8840 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6934 - c_loss: 0.0002 - target_acc: 0.9028 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6921 - c_loss: 0.0002 - target_acc: 0.9196 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6873 - c_loss: 0.0002 - target_acc: 0.8996 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6889 - c_loss: 0.0002 - target_acc: 0.8748 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6868 - c_loss: 0.0003 - target_acc: 0.8996 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6908 - c_loss: 0.0002 - target_acc: 0.9344 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6957 - c_loss: 0.0001 - target_acc: 0.9008 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6978 - c_loss: 0.0003 - target_acc: 0.8648 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6950 - c_loss: 0.0003 - target_acc: 0.9016 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6849 - c_loss: 0.0004 - target_acc: 0.9292 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6868 - c_loss: 0.0002 - target_acc: 0.8888 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6966 - c_loss: 0.0001 - target_acc: 0.8812 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.7004 - c_loss: 0.0004 - target_acc: 0.9104 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6918 - c_loss: 0.0006 - target_acc: 0.9284 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6856 - c_loss: 0.0001 - target_acc: 0.8984 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6900 - c_loss: 0.0006 - target_acc: 0.8556 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6999 - c_loss: 0.0001 - target_acc: 0.9076 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6884 - c_loss: 0.0006 - target_acc: 0.9372 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6930 - c_loss: 0.0001 - target_acc: 0.8860 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6942 - c_loss: 0.0003 - target_acc: 0.8656 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6953 - c_loss: 0.0003 - target_acc: 0.9260 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6885 - c_loss: 0.0001 - target_acc: 0.9260 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6946 - c_loss: 0.0004 - target_acc: 0.8652 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.7007 - c_loss: 0.0001 - target_acc: 0.8952 - source_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch         - d_loss: 0.6957 - c_loss: 0.0001 - target_acc: 0.9304 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6883 - c_loss: 0.0005 - target_acc: 0.9008 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6880 - c_loss: 0.0002 - target_acc: 0.8500 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6924 - c_loss: 0.0003 - target_acc: 0.8968 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6880 - c_loss: 0.0004 - target_acc: 0.9184 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6920 - c_loss: 0.0009 - target_acc: 0.8776 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6962 - c_loss: 0.0001 - target_acc: 0.8880 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6956 - c_loss: 0.0001 - target_acc: 0.9292 - source_acc: 1.0000\n",
      "Epoch         - d_loss: 0.6892 - c_loss: 0.0004 - target_acc: 0.9120 - source_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# train DANN model\n",
    "print('Training DANN model..')\n",
    "for i in range(num_epochs):\n",
    "    source_gen = batch_generator(int(batch_size / 2), Xs_train, ys_train)\n",
    "    target_gen = batch_generator(int(batch_size / 2), Xt_train, None)\n",
    "\n",
    "    # iterate over batches\n",
    "    for (xs, ys, _) in source_gen:\n",
    "        p = float(j) / num_steps\n",
    "        lambd = round(2. / (1. + np.exp(-10. * p)) - 1, 3)\n",
    "        lr = 0.01 / (1. + 100 * p)**0.75\n",
    "        d_clf.set_lambda(lambd)\n",
    "        d_optimizer.lr = lr\n",
    "        c_optimizer.lr = lr\n",
    "        f_optimizer.lr = lr\n",
    "        j += 1\n",
    "        \n",
    "        # get next target batch\n",
    "        xt, _ = next(target_gen)\n",
    "\n",
    "        # exit when batch size mismatch\n",
    "        if len(xs) + len(xt) != batch_size:\n",
    "            print('aa')\n",
    "            continue\n",
    "        \n",
    "        # concatenate source and target batch\n",
    "        x = torch.cat([xs, xt], 0)\n",
    "        \n",
    "        # 1) train feature_extractor and class_classifier on source batch\n",
    "        # reset gradients\n",
    "        f_ext.zero_grad()\n",
    "        c_clf.zero_grad()\n",
    "        \n",
    "        # calculate class_classifier predictions on batch xs\n",
    "        c_out = c_clf(f_ext(xs.float()).view(int(batch_size / 2), -1))\n",
    "\n",
    "        # optimize feature_extractor and class_classifier on output\n",
    "        f_c_loss = c_crit(c_out, ys.float())\n",
    "        f_c_loss.backward(retain_graph = True)\n",
    "        c_optimizer.step()\n",
    "        f_optimizer.step()\n",
    "\n",
    "        # 2) train feature_extractor and domain_classifier on full batch x\n",
    "        # reset gradients\n",
    "        f_ext.zero_grad()\n",
    "        d_clf.zero_grad()\n",
    "        \n",
    "        # calculate domain_classifier predictions on batch x\n",
    "        f_ext(x.float())\n",
    "        d_out = d_clf(f_ext(x.float()).view(batch_size, -1))\n",
    "        \n",
    "        # use normal gradients to optimize domain_classifier\n",
    "        f_d_loss = d_crit(d_out, yd.float())\n",
    "        f_d_loss.backward(retain_graph = True)\n",
    "        d_optimizer.step()\n",
    "        f_optimizer.step()\n",
    "        \n",
    "        # print batch statistics\n",
    "        print('\\rEpoch         - d_loss: {} - c_loss: {}'.format(format(f_d_loss.data[0], '.4f'),\n",
    "                            format(f_c_loss.data[0], '.4f')), end='')           \n",
    "    \n",
    "    # print epoch statistics\n",
    "    t_acc = eval_func(Xt_train, yt_train)\n",
    "    s_acc = eval_func(Xs_train, ys_train)\n",
    "    #t_acc = eval_clf(c_clf, f_ext, Xt_test, yt_test, Xt_test[0])\n",
    "    #s_acc = eval_clf(c_clf, f_ext, Xs_test, ys_test, Xs_test[0])\n",
    "    print(' - target_acc: {} - source_acc: {}'.format(format(t_acc, '.4f'), format(s_acc, '.4f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xs_train = aa['docvec'][aa['st_label']==0][:train_size]\n",
    "ys_train = (aa['true_label'][aa['st_label']==0][:train_size] + 1)/2\n",
    "Xt_train = aa['docvec'][aa['st_label']==1][:train_size]\n",
    "yt_train = (aa['true_label'][aa['st_label']==1][:train_size] + 1)/2\n",
    "Xs_test = aa['docvec'][aa['st_label']==0][-test_size:]\n",
    "ys_test = (aa['true_label'][aa['st_label']==0][-test_size:] +1 )/2\n",
    "Xt_test = aa['docvec'][aa['st_label']==1][-test_size:]\n",
    "yt_test = (aa['true_label'][aa['st_label']==1][-test_size:] +1 )/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
